{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision.models.vgg import VGG\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "train_dir = '/home/masonmcgough/.kaggle/competitions/whale-categorization-playground/train'\n",
    "test_dir = '/home/masonmcgough/.kaggle/competitions/whale-categorization-playground/test'\n",
    "data_path = '/home/masonmcgough/.kaggle/competitions/whale-categorization-playground/train.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make dataset and dataloader classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhaleData(Dataset):\n",
    "    def __init__(self, csv_path, train_dir, test_dir, norm_constants=None):\n",
    "        self.dataframe = pd.read_csv(csv_path)\n",
    "        self.unique_labels, self.label_indices, self.label_counts = np.unique(\n",
    "            self.dataframe.Id, return_inverse=True, return_counts=True)\n",
    "        self.dataframe.insert(len(self.dataframe.columns), \n",
    "                              'label_index', \n",
    "                              self.label_indices)\n",
    "        \n",
    "        self.img_paths = [os.path.join(train_dir, f) for f in self.dataframe.Image]\n",
    "        \n",
    "        if norm_constants is None:\n",
    "            np_means = np.zeros((len(self.img_paths), 3))\n",
    "            np_stds = np.zeros_like(np_means)\n",
    "            for i, img_name in enumerate(self.img_paths):\n",
    "                img = np.array(Image.open(img_name))\n",
    "                np_means[i] = np.mean(img, axis=(0, 1))\n",
    "                np_stds[i] = np.std(img, axis=(0, 1))\n",
    "\n",
    "            self.norm_constants = {'means': np.mean(np_means, axis=0),\n",
    "                              'stds': np.std(np_stds, axis=0)}\n",
    "        else:\n",
    "            self.norm_constants = norm_constants\n",
    "\n",
    "        self.transforms = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Grayscale(num_output_channels=3),\n",
    "            torchvision.transforms.Resize((224, 224)),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize(self.norm_constants['means'], self.norm_constants['stds'])\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.img_paths[idx])\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        \n",
    "        label = torch.tensor(self.label_indices[idx], dtype=torch.long)\n",
    "        return img, label\n",
    "    \n",
    "batch_size = 8\n",
    "num_workers = 1\n",
    "norm_constants = {'means': [140.28481434, 147.81499958, 156.63975966], \n",
    "                  'stds': [17.15088799, 17.18349816, 17.21105706]}\n",
    "dataset = WhaleData(data_path, train_dir, test_dir, norm_constants=norm_constants)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGNet(VGG):\n",
    "    vgg_ranges = {\n",
    "        'vgg11': ((0, 3), (3, 6),  (6, 11),  (11, 16), (16, 21)),\n",
    "        'vgg13': ((0, 5), (5, 10), (10, 15), (15, 20), (20, 25)),\n",
    "        'vgg16': ((0, 5), (5, 10), (10, 17), (17, 24), (24, 31)),\n",
    "        'vgg19': ((0, 5), (5, 10), (10, 19), (19, 28), (28, 37))\n",
    "    }\n",
    "\n",
    "    # cropped version from https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py\n",
    "    cfg = {\n",
    "        'vgg11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "        'vgg13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "        'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "        'vgg19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "    }\n",
    "    \n",
    "    def __init__(self, pretrained=True, model='vgg16', requires_grad=True, remove_fc=True, show_params=False):\n",
    "        super().__init__(self.make_layers(self.cfg[model]))\n",
    "        self.ranges = self.vgg_ranges[model]\n",
    "\n",
    "        if pretrained:\n",
    "            exec(\"self.load_state_dict(torchvision.models.%s(pretrained=True).state_dict())\" % model)\n",
    "\n",
    "        if not requires_grad:\n",
    "            for param in super().parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        if remove_fc:  # delete redundant fully-connected layer params, can save memory\n",
    "            del self.classifier\n",
    "\n",
    "        if show_params:\n",
    "            for name, param in self.named_parameters():\n",
    "                print(name, param.size())\n",
    "                \n",
    "        self.set_device()\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = {}\n",
    "\n",
    "        # get the output of each maxpooling layer (5 maxpool in VGG net)\n",
    "        for idx in range(len(self.ranges)):\n",
    "            for layer in range(self.ranges[idx][0], self.ranges[idx][1]):\n",
    "                x = self.features[layer](x)\n",
    "            output[\"x%d\"%(idx+1)] = x\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def train(self, dataloader, num_epochs=25, validation=False, disp_interval=None, use_visdom=False):\n",
    "        self.to(self.device)\n",
    "\n",
    "        if validation:\n",
    "            phase = 'Validation'\n",
    "        else:\n",
    "            phase = 'Training'\n",
    "\n",
    "        if use_visdom:\n",
    "            vis = visdom.Visdom()\n",
    "            gen = Generator(self, dataloader.dataset)\n",
    "        else:\n",
    "            vis = None\n",
    "\n",
    "        losses = []\n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            if not validation:\n",
    "                self.scheduler.step()\n",
    "                super().train()\n",
    "            else:\n",
    "                self.eval()\n",
    "\n",
    "            # reset loss for current phase and epoch\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for inputs, labels in dataloader:\n",
    "                inputs = inputs.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                print('Input shape: {}'.format(inputs.size))\n",
    "                print('Label shape: {}'.format(labels.size))\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # track history only during training phase\n",
    "                with torch.set_grad_enabled(not validation):\n",
    "                    outputs = self(inputs)\n",
    "                    loss = self.criterion(outputs, labels)\n",
    "\n",
    "                    if not validation:\n",
    "                        loss.backward()\n",
    "                        self.optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            losses.append(running_loss)\n",
    "            if disp_interval is not None and epoch % disp_interval == 0:\n",
    "                epoch_loss = running_loss / len(dataloader)\n",
    "                print('Epoch {} / {}'.format(epoch, num_epochs))\n",
    "                print('Learning Rate: {}'.format(self.scheduler.get_lr()))\n",
    "                print('{} Loss: {}'.format(phase, epoch_loss))\n",
    "                print('-' * 10)\n",
    "                print()\n",
    "\n",
    "    def make_layers(self, cfg, batch_norm=False):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for v in cfg:\n",
    "            if v == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "                if batch_norm:\n",
    "                    layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "                else:\n",
    "                    layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "                in_channels = v\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def set_device(self, device=None):\n",
    "        if device is None:\n",
    "            self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        else:\n",
    "            self.device = device\n",
    "\n",
    "\n",
    "model = VGGNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define optimizer, loss, and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create class weights to balance class frequencies\n",
    "print('Total images: {}'.format(np.sum(dataset.label_counts)))\n",
    "print('Number per label: {}'.format(dataset.label_counts))\n",
    "\n",
    "class_weights = np.max(dataset.label_counts) - dataset.label_counts + 1\n",
    "class_weights = class_weights / np.sum(class_weights)\n",
    "print('Class weights: {}'.format(class_weights))\n",
    "print('Total probability: {}'.format(np.sum(class_weights)))\n",
    "\n",
    "class_weights = torch.tensor(class_weights)\n",
    "print('\\nClass weights tensor: {}'.format(class_weights))\n",
    "print('\\tSize: {}'.format(class_weights.size()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_rate = 0.001\n",
    "momentum = 0.9\n",
    "step_size = 50\n",
    "gamma = 0.8\n",
    "\n",
    "model.criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "model.optimizer = torch.optim.SGD(model.parameters(), lr=learn_rate, momentum=momentum)\n",
    "model.scheduler = torch.optim.lr_scheduler.StepLR(model.optimizer, step_size=step_size, gamma=gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(dataloader, disp_interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PyTorchSIIM]",
   "language": "python",
   "name": "conda-env-PyTorchSIIM-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
